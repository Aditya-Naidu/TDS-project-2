Index: utils/solution_functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import requests\nimport subprocess\nimport hashlib\nimport numpy as np\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime, timedelta\nimport zipfile\nimport pandas as pd\nimport os\nimport gzip\nimport re\n\nfrom utils.file_process import unzip_folder\n\n\ndef vs_code_version():\n    return \"\"\"\n    Version:          Code 1.98.2 (ddc367ed5c8936efe395cffeec279b04ffd7db78, 2025-03-12T13:32:45.399Z)\n    OS Version:       Linux x64 6.12.15-200.fc41.x86_64\n    CPUs:             11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (8 x 1300)\n    Memory (System):  7.40GB (3.72GB free)\n    Load (avg):       3, 2, 2\n    VM:               0%\n    Screen Reader:    no\n    Process Argv:     --crash-reporter-id 80b4d7e7-0056-4767-b601-6fcdbec0b54d\n    GPU Status:       2d_canvas:                              enabled\n                    canvas_oop_rasterization:               enabled_on\n                    direct_rendering_display_compositor:    disabled_off_ok\n                    gpu_compositing:                        enabled\n                    multiple_raster_threads:                enabled_on\n                    opengl:                                 enabled_on\n                    rasterization:                          enabled\n                    raw_draw:                               disabled_off_ok\n                    skia_graphite:                          disabled_off\n                    video_decode:                           enabled\n                    video_encode:                           disabled_software\n                    vulkan:                                 disabled_off\n                    webgl:                                  enabled\n                    webgl2:                                 enabled\n                    webgpu:                                 disabled_off\n                    webnn:                                  disabled_off\n\n    CPU %\tMem MB\t   PID\tProcess\n        2\t   189\t 18772\tcode main\n        0\t    45\t 18800\t   zygote\n        2\t   121\t 19189\t     gpu-process\n        0\t    45\t 18801\t   zygote\n        0\t     8\t 18825\t     zygote\n        0\t    61\t 19199\t   utility-network-service\n        0\t   106\t 20078\tptyHost\n        2\t   114\t 20116\textensionHost [1]\n    21\t   114\t 20279\tshared-process\n        0\t     0\t 20778\t     /usr/bin/zsh -i -l -c '/usr/share/code/code'  -p '\"0c1d701e5812\" + JSON.stringify(process.env) + \"0c1d701e5812\"'\n        0\t    98\t 20294\tfileWatcher [1]\n\n    Workspace Stats:\n    |  Window (● solutions.py - tdsproj2 - python - Visual Studio Code)\n    |    Folder (tdsproj2): 6878 files\n    |      File types: py(3311) pyc(876) pyi(295) so(67) f90(60) txt(41) typed(36)\n    |                  csv(31) h(28) f(23)\n    |      Conf files:\n    \"\"\"\n\n\ndef make_http_requests_with_uv(url=\"https://httpbin.org/get\", query_params={\"email\": \"23f2005217@ds.study.iitm.ac.in\"}):\n    print(url)\n    try:\n        response = requests.get(url, params=query_params)\n        return response.json()\n    except requests.RequestException as e:\n        print(f\"HTTP request failed: {e}\")\n        return None\n\ndef run_command_with_npx(arguments):\n    filePath, prettier_version, hash_algo, use_npx = (\n        \"README.md\",\n        \"3.4.2\",\n        \"sha256\",\n        True,\n    )\n    filePath, prettier_version, hash_algo, use_npx = (\n        arguments[\"filePath\"],\n        arguments[\"prettier_version\"],\n        arguments[\"hash_algo\"],\n        arguments[\"use_npx\"],\n    )\n    prettier_cmd = (\n        [\"npx\", \"-y\", f\"prettier@{prettier_version}\", filePath]\n        if use_npx\n        else [\"prettier\", filePath]\n    )\n\n    try:\n        prettier_process = subprocess.run(\n            prettier_cmd, capture_output=True, text=True, check=True\n        )\n    except subprocess.CalledProcessError as e:\n        print(\"Error running Prettier:\", e)\n        return None\n\n    formatted_content = prettier_process.stdout.encode()\n\n    try:\n        hasher = hashlib.new(hash_algo)\n        hasher.update(formatted_content)\n        return hasher.hexdigest()\n    except ValueError:\n        print(f\"Invalid hash algorithm: {hash_algo}\")\n        return None\n\n\ndef use_google_sheets(rows=100, cols=100, start=15, step=12, extract_rows=1, extract_cols=10):\n    matrix = np.arange(start, start + (rows * cols * step),\n                       step).reshape(rows, cols)\n\n    extracted_values = matrix[:extract_rows, :extract_cols]\n\n    return np.sum(extracted_values)\n\n\ndef calculate_spreadsheet_formula(formula: str, type: str) -> str:\n    try:\n        if formula.startswith(\"=\"):\n            formula = formula[1:]\n\n        if \"SEQUENCE\" in formula and type == \"google_sheets\":\n            # Example: SUM(ARRAY_CONSTRAIN(SEQUENCE(100, 100, 5, 2), 1, 10))\n            sequence_pattern = r\"SEQUENCE\\((\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\)\"\n            match = re.search(sequence_pattern, formula)\n\n            if match:\n                rows = int(match.group(1))\n                cols = int(match.group(2))\n                start = int(match.group(3))\n                step = int(match.group(4))\n\n                # Generate the sequence\n                sequence = []\n                value = start\n                for _ in range(rows):\n                    row = []\n                    for _ in range(cols):\n                        row.append(value)\n                        value += step\n                    sequence.append(row)\n\n                # Check for ARRAY_CONSTRAIN\n                constrain_pattern = r\"ARRAY_CONSTRAIN\\([^,]+,\\s*(\\d+),\\s*(\\d+)\\)\"\n                constrain_match = re.search(constrain_pattern, formula)\n\n                if constrain_match:\n                    constrain_rows = int(constrain_match.group(1))\n                    constrain_cols = int(constrain_match.group(2))\n\n                    # Apply constraints\n                    constrained = []\n                    for i in range(min(constrain_rows, len(sequence))):\n                        row = sequence[i][:constrain_cols]\n                        constrained.extend(row)\n\n                    if \"SUM(\" in formula:\n                        return str(sum(constrained))\n\n        elif \"SORTBY\" in formula and type == \"excel\":\n            # Example: SUM(TAKE(SORTBY({1,10,12,4,6,8,9,13,6,15,14,15,2,13,0,3}, {10,9,13,2,11,8,16,14,7,15,5,4,6,1,3,12}), 1, 6))\n\n            # Extract the arrays from SORTBY\n            arrays_pattern = r\"SORTBY\\(\\{([^}]+)\\},\\s*\\{([^}]+)\\}\\)\"\n            arrays_match = re.search(arrays_pattern, formula)\n\n            if arrays_match:\n                values = [int(x.strip())\n                          for x in arrays_match.group(1).split(\",\")]\n                sort_keys = [int(x.strip())\n                             for x in arrays_match.group(2).split(\",\")]\n\n                # Sort the values based on sort_keys\n                sorted_pairs = sorted(\n                    zip(values, sort_keys), key=lambda x: x[1])\n                sorted_values = [pair[0] for pair in sorted_pairs]\n\n                # Check for TAKE\n                take_pattern = r\"TAKE\\([^,]+,\\s*(\\d+),\\s*(\\d+)\\)\"\n                take_match = re.search(take_pattern, formula)\n\n                if take_match:\n                    take_start = int(take_match.group(1))\n                    take_count = int(take_match.group(2))\n\n                    # Apply TAKE function\n                    taken = sorted_values[take_start -\n                                          1: take_start - 1 + take_count]\n\n                    # Check for SUM\n                    if \"SUM(\" in formula:\n                        return str(sum(taken))\n\n        return \"Could not parse the formula or unsupported formula type\"\n\n    except Exception as e:\n        return f\"Error calculating spreadsheet formula: {str(e)}\"\n\n\ndef use_excel(values=None, sort_keys=None, num_rows=1, num_elements=9):\n    if values is None:\n        values = np.array(\n            [13, 12, 0, 14, 2, 12, 9, 15, 1, 7, 3, 10, 9, 15, 2, 0])\n    if sort_keys is None:\n        sort_keys = np.array(\n            [10, 9, 13, 2, 11, 8, 16, 14, 7, 15, 5, 4, 6, 1, 3, 12])\n\n    sorted_values = values[np.argsort(sort_keys)]\n    return np.sum(sorted_values[:num_elements])\n\n\ndef use_devtools(html=None, input_name=None):\n    if html is None:\n        html = '<input type=\"hidden\" name=\"secret\" value=\"12345\">'\n    if input_name is None:\n        input_name = \"secret\"\n\n    soup = BeautifulSoup(html, \"html.parser\")\n    hidden_input = soup.find(\"input\", {\"type\": \"hidden\", \"name\": input_name})\n\n    return hidden_input[\"value\"] if hidden_input else None\n\n\ndef count_wednesdays(start_date=\"1990-04-08\", end_date=\"2008-09-29\", weekday=2):\n    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n    count = sum(\n        1\n        for _ in range((end - start).days + 1)\n        if (start + timedelta(_)).weekday() == weekday\n    )\n    return count\n\n\ndef extract_csv_from_a_zip(\n    zip_path,\n    extract_to=\"extracted_files\",\n    csv_filename=\"extract.csv\",\n    column_name=\"answer\",\n):\n    os.makedirs(extract_to, exist_ok=True)\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n\n    csv_path = os.path.join(extract_to, csv_filename)\n\n    if not os.path.exists(csv_path):\n        for root, _, files in os.walk(extract_to):\n            for file in files:\n                if file.lower().endswith(\".csv\"):\n                    csv_path = os.path.join(root, file)\n                    break\n\n    if os.path.exists(csv_path):\n        df = pd.read_csv(csv_path)\n        if column_name in df.columns:\n            return \", \".join(map(str, df[column_name].dropna().tolist()))\n\n    return \"\"\n\n\ndef use_json():\n    return \"\"\n\n\ndef multi_cursor_edits_to_convert_to_json():\n    return \"\"\n\n\ndef css_selectors():\n    return \"\"\n\n\ndef process_files_with_different_encodings():\n    return \"\"\n\n\ndef use_github():\n    return \"\"\n\n\ndef replace_across_files():\n    return \"\"\n\n\ndef list_files_and_attributes():\n    return \"\"\n\n\ndef move_and_rename_files():\n    return \"\"\n\n\ndef compare_files():\n    return \"\"\n\n\ndef sql_ticket_sales():\n    return \"\"\n\n\ndef write_documentation_in_markdown():\n    return \"\"\n\n\ndef compress_an_image():\n    return \"\"\n\n\ndef host_your_portfolio_on_github_pages():\n    return \"\"\n\n\ndef use_google_colab():\n    return \"\"\n\n\ndef use_an_image_library_in_google_colab():\n    return \"\"\n\n\ndef deploy_a_python_api_to_vercel():\n    return \"\"\n\n\ndef create_a_github_action():\n    return \"\"\n\n\ndef push_an_image_to_docker_hub():\n    return \"\"\n\n\ndef write_a_fastapi_server_to_serve_data():\n    return \"\"\n\n\ndef run_a_local_llm_with_llamafile():\n    return \"\"\n\n\ndef llm_sentiment_analysis():\n    return \"\"\n\n\ndef llm_token_cost():\n    return \"\"\n\n\ndef generate_addresses_with_llms():\n    return \"\"\n\n\ndef llm_vision():\n    return \"\"\n\n\ndef llm_embeddings():\n    return \"\"\n\n\ndef embedding_similarity():\n    return \"\"\n\n\ndef vector_databases():\n    return \"\"\n\n\ndef function_calling():\n    return \"\"\n\n\ndef get_an_llm_to_say_yes():\n    return \"\"\n\n\ndef import_html_to_google_sheets():\n    return \"\"\n\n\ndef scrape_imdb_movies():\n    return \"\"\n\n\ndef wikipedia_outline():\n    return \"\"\n\n\ndef scrape_the_bbc_weather_api():\n    return \"\"\n\n\ndef find_the_bounding_box_of_a_city():\n    return \"\"\n\n\ndef search_hacker_news():\n    return \"\"\n\n\ndef find_newest_github_user():\n    return \"\"\n\n\ndef create_a_scheduled_github_action():\n    return \"\"\n\n\ndef extract_tables_from_pdf():\n    return \"\"\n\n\ndef convert_a_pdf_to_markdown():\n    return \"\"\n\n\ndef clean_up_excel_sales_data():\n    return \"\"\n\n\ndef parse_log_line(line):\n    # Regex for parsing log lines\n    log_pattern = (\n        r'^(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\d+) (\\S+) \"(.*?)\" \"(.*?)\" (\\S+) (\\S+)$')\n    match = re.match(log_pattern, line)\n    if match:\n        return {\n            \"ip\": match.group(1),\n            \"time\": match.group(4),  # e.g. 01/May/2024:00:00:00 -0500\n            \"method\": match.group(5),\n            \"url\": match.group(6),\n            \"protocol\": match.group(7),\n            \"status\": int(match.group(8)),\n            \"size\": int(match.group(9)) if match.group(9).isdigit() else 0,\n            \"referer\": match.group(10),\n            \"user_agent\": match.group(11),\n            \"vhost\": match.group(12),\n            \"server\": match.group(13)\n        }\n    return None\n\n\ndef load_logs(file_path):\n    if not os.path.exists(file_path):\n        print(f\"Error: File '{file_path}' not found.\")\n        return pd.DataFrame()\n\n    parsed_logs = []\n    # Open with errors='ignore' for problematic lines\n    with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:\n        for line in f:\n            parsed_entry = parse_log_line(line)\n            if parsed_entry:\n                parsed_logs.append(parsed_entry)\n    return pd.DataFrame(parsed_logs)\n\n\ndef convert_time(timestamp):\n    return datetime.strptime(timestamp, \"%d/%b/%Y:%H:%M:%S %z\")\n\n\ndef clean_up_student_marks(file_path, section_prefix, weekday, start_hour, end_hour, month, year):\n    \"\"\"\n    Analyzes the logs to count the number of successful GET requests.\n\n    Parameters:\n    - file_path: path to the GZipped log file.\n    - section_prefix: URL prefix to filter (e.g., \"/telugu/\" or \"/tamilmp3/\").\n    - weekday: integer (0=Monday, ..., 6=Sunday).\n    - start_hour: start time (inclusive) in 24-hour format.\n    - end_hour: end time (exclusive) in 24-hour format.\n    - month: integer month (e.g., 5 for May).\n    - year: integer year (e.g., 2024).\n\n    Returns:\n    - Count of successful GET requests matching the criteria.\n    \"\"\"\n    df = load_logs(file_path)\n    if df.empty:\n        print(\"No log data available for processing.\")\n        return 0\n\n    # Convert time field to datetime\n    df[\"datetime\"] = df[\"time\"].apply(convert_time)\n\n    # Filter for the specific month and year\n    df = df[(df[\"datetime\"].dt.month == month)\n            & (df[\"datetime\"].dt.year == year)]\n\n    # Filter for the specific day of the week\n    df = df[df[\"datetime\"].dt.weekday == weekday]\n\n    # Filter for the specific time window\n    df = df[(df[\"datetime\"].dt.hour >= start_hour)\n            & (df[\"datetime\"].dt.hour < end_hour)]\n\n    # Apply filters for GET requests, URL prefix, and successful status codes\n    filtered_df = df[\n        (df[\"method\"] == \"GET\") &\n        (df[\"url\"].str.startswith(section_prefix)) &\n        (df[\"status\"].between(200, 299))\n    ]\n\n    return filtered_df.shape[0]\n\n\ndef apache_log_requests():\n    return \"\"\n\n\ndef apache_log_downloads():\n    return \"\"\n\n\ndef clean_up_sales_data():\n    return \"\"\n\n\ndef parse_partial_json():\n    return \"\"\n\n\ndef extract_nested_json_keys():\n    return \"\"\n\n\ndef duckdb_social_media_interactions():\n    return \"\"\n\n\ndef transcribe_a_youtube_video():\n    return \"\"\n\n\ndef reconstruct_an_image():\n    return \"\"\n\n\nfunctions_dict = {\n    \"vs_code_version\": vs_code_version,\n    \"make_http_requests_with_uv\": make_http_requests_with_uv,\n    \"run_command_with_npx\": run_command_with_npx,\n    \"use_google_sheets\": use_google_sheets,\n    \"use_excel\": use_excel,\n    \"use_devtools\": use_devtools,\n    \"count_wednesdays\": count_wednesdays,\n    \"extract_csv_from_a_zip\": extract_csv_from_a_zip,\n    \"use_json\": use_json,\n    \"multi_cursor_edits_to_convert_to_json\": multi_cursor_edits_to_convert_to_json,\n    \"css_selectors\": css_selectors,\n    \"process_files_with_different_encodings\": process_files_with_different_encodings,\n    \"use_github\": use_github,\n    \"replace_across_files\": replace_across_files,\n    \"list_files_and_attributes\": list_files_and_attributes,\n    \"move_and_rename_files\": move_and_rename_files,\n    \"compare_files\": compare_files,\n    \"sql_ticket_sales\": sql_ticket_sales,\n    \"write_documentation_in_markdown\": write_documentation_in_markdown,\n    \"compress_an_image\": compress_an_image,\n    \"host_your_portfolio_on_github_pages\": host_your_portfolio_on_github_pages,\n    \"use_google_colab\": use_google_colab,\n    \"use_an_image_library_in_google_colab\": use_an_image_library_in_google_colab,\n    \"deploy_a_python_api_to_vercel\": deploy_a_python_api_to_vercel,\n    \"create_a_github_action\": create_a_github_action,\n    \"push_an_image_to_docker_hub\": push_an_image_to_docker_hub,\n    \"write_a_fastapi_server_to_serve_data\": write_a_fastapi_server_to_serve_data,\n    \"run_a_local_llm_with_llamafile\": run_a_local_llm_with_llamafile,\n    \"llm_sentiment_analysis\": llm_sentiment_analysis,\n    \"llm_token_cost\": llm_token_cost,\n    \"generate_addresses_with_llms\": generate_addresses_with_llms,\n    \"llm_vision\": llm_vision,\n    \"llm_embeddings\": llm_embeddings,\n    \"embedding_similarity\": embedding_similarity,\n    \"vector_databases\": vector_databases,\n    \"function_calling\": function_calling,\n    \"get_an_llm_to_say_yes\": get_an_llm_to_say_yes,\n    \"import_html_to_google_sheets\": import_html_to_google_sheets,\n    \"scrape_imdb_movies\": scrape_imdb_movies,\n    \"wikipedia_outline\": wikipedia_outline,\n    \"scrape_the_bbc_weather_api\": scrape_the_bbc_weather_api,\n    \"find_the_bounding_box_of_a_city\": find_the_bounding_box_of_a_city,\n    \"search_hacker_news\": search_hacker_news,\n    \"find_newest_github_user\": find_newest_github_user,\n    \"create_a_scheduled_github_action\": create_a_scheduled_github_action,\n    \"extract_tables_from_pdf\": extract_tables_from_pdf,\n    \"convert_a_pdf_to_markdown\": convert_a_pdf_to_markdown,\n    \"clean_up_excel_sales_data\": clean_up_excel_sales_data,\n    \"clean_up_student_marks\": clean_up_student_marks,\n    \"apache_log_requests\": apache_log_requests,\n    \"apache_log_downloads\": apache_log_downloads,\n    \"clean_up_sales_data\": clean_up_sales_data,\n    \"parse_partial_json\": parse_partial_json,\n    \"extract_nested_json_keys\": extract_nested_json_keys,\n    \"duckdb_social_media_interactions\": duckdb_social_media_interactions,\n    \"transcribe_a_youtube_video\": transcribe_a_youtube_video,\n    \"reconstruct_an_image\": reconstruct_an_image,\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/utils/solution_functions.py b/utils/solution_functions.py
--- a/utils/solution_functions.py	(revision 09f529cb700a09b44ba78bb1f7bdfcf456c37288)
+++ b/utils/solution_functions.py	(date 1742991017810)
@@ -409,15 +409,49 @@
 
 
 def create_a_scheduled_github_action():
-    return ""
+    return "Triggered Github Actions Solution"
+
+
+def extract_tables_from_pdf(pdf_path):
+    import sys
+    import pandas as pd
+    try:
+        from tabula.io import read_pdf
+    except ImportError as ie:
+        return "Error: tabula-py is not installed. Please install it using 'pip install tabula-py'."
+
+    try:
+        # Read all tables from the PDF; returns a list of DataFrames.
+        tables = read_pdf(pdf_path, pages='all', multiple_tables=True)
+    except Exception as e:
+        return f"Failed to read PDF: {e}"
 
+    if not tables:
+        return "No tables found in the PDF file."
 
-def extract_tables_from_pdf():
-    return ""
+    # Assume the first table is the target table.
+    df = tables[0]
+
+    # Check for expected columns.
+    expected_columns = ['Group', 'Maths', 'Economics']
+    missing_cols = [col for col in expected_columns if col not in df.columns]
+    if missing_cols:
+        return f"Expected columns not found in DataFrame: {missing_cols}"
+
+    # Convert columns to numeric.
+    for col in expected_columns:
+        df[col] = pd.to_numeric(df[col], errors='coerce')
+
+    # Filter rows: groups 3-29 and Economics marks >= 19.
+    filtered_df = df[(df['Group'] >= 3) & (df['Group'] <= 29) & (df['Economics'] >= 19)]
+
+    # Sum the Maths marks.
+    total_maths = filtered_df['Maths'].sum()
+    return total_maths
 
 
 def convert_a_pdf_to_markdown():
-    return ""
+    return "Triggered Markdown"
 
 
 def clean_up_excel_sales_data():
Index: tests/test_api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import requests\nimport zipfile\nimport io\n\nurl = \"http://127.0.0.1:5000\"\nquestions = [\n    {\n        \"files\": None,\n        \"question\": \"\"\"Running uv run --with httpie -- https [URL] installs the Python package httpie and sends a HTTPS request to the URL.\\n    Send a HTTPS request to https://httpbin.org/get with the URL encoded parameter email set to 23f2005217@ds.study.iitm.ac.in\\n    What is the JSON output of the command? (Paste only the JSON body, not the headers)\"\"\",\n    },\n    # {\n    #     \"files\": \"tests/files/README.md\",\n    #     \"question\": \"Let's make sure you know how to use npx and prettier.\\n    Download . In the directory where you downloaded it, make sure it is called README.md, and run npx -y prettier@3.4.2 README.md | sha256sum.\\n    What is the output of the command?\",\n    # },\n    # {\n    #     \"question\": \"Install and run Visual Studio Code. In your Terminal (or Command Prompt), type code -s and press Enter. Copy and paste the entire output below.\\n    What is the output of code -s?\"\n    # }\n]\n\nfor question in questions:\n    data = {\"question\": question.get(\"question\")}\n    files = {\"file\": open(question.get(\"files\"), \"rb\")} if question.get(\"files\") else None\n    response = requests.post(url, data=data, files=files)\n    print(response.json())\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tests/test_api.py b/tests/test_api.py
--- a/tests/test_api.py	(revision 09f529cb700a09b44ba78bb1f7bdfcf456c37288)
+++ b/tests/test_api.py	(date 1742991173262)
@@ -5,9 +5,32 @@
 url = "http://127.0.0.1:5000"
 questions = [
     {
+        "files": "/Users/adityanaidu/Documents/scripts/StudyIIT/TDS-project-2/tests/files/q-pdf-to-markdown.pdf",
+        "question": """ What is the markdown content of the PDF, formatted with prettier@3.4.2?""",
+    },
+    {
         "files": None,
-        "question": """Running uv run --with httpie -- https [URL] installs the Python package httpie and sends a HTTPS request to the URL.\n    Send a HTTPS request to https://httpbin.org/get with the URL encoded parameter email set to 23f2005217@ds.study.iitm.ac.in\n    What is the JSON output of the command? (Paste only the JSON body, not the headers)""",
+        "question": """ Create a scheduled GitHub action that runs daily and adds a commit to your repository. The workflow should:
+
+Use schedule with cron syntax to run once per day (must use specific hours/minutes, not wildcards)
+Include a step with your email 21f3003062@ds.study.iitm.ac.in in its name
+Create a commit in each run
+Be located in .github/workflows/ directory
+After creating the workflow:
+
+Trigger the workflow and wait for it to complete
+Ensure it appears as the most recent action in your repository
+Verify that it creates a commit during or within 5 minutes of the workflow run
+Enter your repository URL (format: https://github.com/USER/REPO):""",
     },
+    {
+        "files": "/Users/adityanaidu/Documents/scripts/StudyIIT/TDS-project-2/tests/files/q-extract-tables-from-pdf.pdf",
+        "question": """ What is the total Maths marks of students who scored 19 or more marks in Economics in groups 3-29 (including both groups)?""",
+    },
+    # {
+    #     "files": None,
+    #     "question": """Running uv run --with httpie -- https [URL] installs the Python package httpie and sends a HTTPS request to the URL.\n    Send a HTTPS request to https://httpbin.org/get with the URL encoded parameter email set to 23f2005217@ds.study.iitm.ac.in\n    What is the JSON output of the command? (Paste only the JSON body, not the headers)""",
+    # },
     # {
     #     "files": "tests/files/README.md",
     #     "question": "Let's make sure you know how to use npx and prettier.\n    Download . In the directory where you downloaded it, make sure it is called README.md, and run npx -y prettier@3.4.2 README.md | sha256sum.\n    What is the output of the command?",
Index: api/app.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from multiprocessing import process\nimport subprocess\nfrom flask import Flask, request, jsonify\nimport os\nfrom utils.question_matching import find_similar_question\nfrom utils.file_process import unzip_folder\nfrom utils.function_definations_llm import function_definitions_objects_llm\nfrom utils.openai_api import extract_parameters\nfrom utils.solution_functions import functions_dict\n\ntmp_dir = \"tmp_uploads\"\nos.makedirs(tmp_dir, exist_ok=True)\n\napp = Flask(__name__)\n\n\nSECRET_PASSWORD = os.getenv(\"SECRET_PASSWORD\")\n\n\n@app.route(\"/\", methods=[\"POST\"])\ndef process_file():\n    question = request.form.get(\"question\")\n    file = request.files.get(\"file\")  # Get the uploaded file (optional)\n    file_names = []\n\n    # Ensure tmp_dir is always assigned\n    tmp_dir = \"tmp_uploads\"\n    try:\n        matched_function, matched_description = find_similar_question(question)\n\n        if file:\n            temp_dir, file_names = unzip_folder(file)\n            tmp_dir = temp_dir  # Update tmp_dir if a file is uploaded\n        parameters = extract_parameters(\n            str(question),\n            function_definitions_llm=function_definitions_objects_llm[matched_function],\n        )\n\n        solution_function = functions_dict.get(\n            str(matched_function), lambda parameters: \"No matching function found\"\n        )\n\n        if file:\n            answer = solution_function(file, *parameters)\n        else:\n            print(type(parameters), parameters)\n            answer = solution_function(*parameters)\n            print(answer)\n        return jsonify({\"answer\": answer})\n    except Exception as e:\n        print(e,\"this is the error\")\n        return jsonify({\"error\": str(e)}), 500\n\n\n@app.route('/redeploy', methods=['GET'])\ndef redeploy():\n    password = request.args.get('password')\n    print(password)\n    print(SECRET_PASSWORD)\n    if password != SECRET_PASSWORD:\n        return \"Unauthorized\", 403\n\n    subprocess.run([\"../redeploy.sh\"], shell=True)\n    return \"Redeployment triggered!\", 200\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/api/app.py b/api/app.py
--- a/api/app.py	(revision 09f529cb700a09b44ba78bb1f7bdfcf456c37288)
+++ b/api/app.py	(date 1742969229072)
@@ -1,54 +1,66 @@
-from multiprocessing import process
+from multiprocessing import Process
 import subprocess
 from flask import Flask, request, jsonify
 import os
+import inspect
 from utils.question_matching import find_similar_question
 from utils.file_process import unzip_folder
 from utils.function_definations_llm import function_definitions_objects_llm
 from utils.openai_api import extract_parameters
 from utils.solution_functions import functions_dict
 
+# Ensure the temporary directory exists
 tmp_dir = "tmp_uploads"
 os.makedirs(tmp_dir, exist_ok=True)
 
 app = Flask(__name__)
-
-
 SECRET_PASSWORD = os.getenv("SECRET_PASSWORD")
 
 
 @app.route("/", methods=["POST"])
 def process_file():
     question = request.form.get("question")
-    file = request.files.get("file")  # Get the uploaded file (optional)
+    file = request.files.get("file")
+    file_path = None  # This will hold the saved file path if a file is uploaded
     file_names = []
+    tmp_dir_local = "tmp_uploads"
 
-    # Ensure tmp_dir is always assigned
-    tmp_dir = "tmp_uploads"
     try:
         matched_function, matched_description = find_similar_question(question)
 
         if file:
-            temp_dir, file_names = unzip_folder(file)
-            tmp_dir = temp_dir  # Update tmp_dir if a file is uploaded
-        parameters = extract_parameters(
-            str(question),
-            function_definitions_llm=function_definitions_objects_llm[matched_function],
-        )
+            # Save the uploaded file to disk
+            file_path = os.path.join(tmp_dir_local, file.filename)
+            file.save(file_path)
+            # Process the file (if not a zip, it will be moved)
+            file_path, file_names = unzip_folder(file_path)
+
+        # Extract parameters using the matched function's definition
+        parameters = extract_parameters(str(question),
+                                        function_definitions_llm=function_definitions_objects_llm[matched_function])
+        if parameters is None:
+            print("No parameters detected, using empty list as parameters")
+            parameters = []
 
         solution_function = functions_dict.get(
-            str(matched_function), lambda parameters: "No matching function found"
+            str(matched_function),
+            lambda *args, **kwargs: "No matching function found"
         )
 
+        # Check the function signature to decide whether to pass the file path
+        sig = inspect.signature(solution_function)
         if file:
-            answer = solution_function(file, *parameters)
+            if len(sig.parameters) == 0:
+                answer = solution_function(*parameters)
+            else:
+                answer = solution_function(file_path, *parameters)
         else:
-            print(type(parameters), parameters)
             answer = solution_function(*parameters)
-            print(answer)
+
+        print(answer)
         return jsonify({"answer": answer})
     except Exception as e:
-        print(e,"this is the error")
+        print(e, "this is the error")
         return jsonify({"error": str(e)}), 500
 
 
@@ -59,10 +71,9 @@
     print(SECRET_PASSWORD)
     if password != SECRET_PASSWORD:
         return "Unauthorized", 403
-
     subprocess.run(["../redeploy.sh"], shell=True)
     return "Redeployment triggered!", 200
 
 
 if __name__ == "__main__":
-    app.run(debug=True)
+    app.run(debug=True)
\ No newline at end of file
Index: utils/function_definations_llm.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>function_definitions_objects_llm = {\n    \"vs_code_version\": {\n        \"name\": \"vs_code_version\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \n            },\n            \"required\": [\"\"]\n        }\n    },\n\n    \"make_http_requests_with_uv\": {\n        \"name\": \"make_http_requests_with_uv\",\n        \"description\": \"extract the http url and query parameters from the given text for example 'uv run --with httpie -- https [URL] installs the Python package httpie and sends a HTTPS request to the URL. Send a HTTPS request to https://httpbin.org/get with the URL encoded parameter country set to India and city set to Chennai. What is the JSON output of the command? (Paste only the JSON body, not the headers)' in this example country: India and city: Chennai are the query parameters\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query_params\": {\n                    \"type\": \"object\",\n                    \"description\": \"The query parameters to send with the request URL encoded parameters\"\n                },\n            },\n            \"required\": [\"query_params\",\"url\"]\n        }\n    },\n\n    \"run_command_with_npx\": {\n        \"name\": \"run_command_with_npx\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_google_sheets\": {\n        \"name\": \"use_google_sheets\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_excel\": {\n        \"name\": \"use_excel\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_devtools\": {\n        \"name\": \"use_devtools\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"count_wednesdays\": {\n        \"name\": \"count_wednesdays\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"extract_csv_from_a_zip\": {\n        \"name\": \"extract_csv_from_a_zip\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_json\": {\n        \"name\": \"use_json\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"multi_cursor_edits_to_convert_to_json\": {\n        \"name\": \"multi_cursor_edits_to_convert_to_json\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"css_selectors\": {\n        \"name\": \"css_selectors\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"process_files_with_different_encodings\": {\n        \"name\": \"process_files_with_different_encodings\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_github\": {\n        \"name\": \"use_github\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"replace_across_files\": {\n        \"name\": \"replace_across_files\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"list_files_and_attributes\": {\n        \"name\": \"list_files_and_attributes\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"move_and_rename_files\": {\n        \"name\": \"move_and_rename_files\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"compare_files\": {\n        \"name\": \"compare_files\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"sql_ticket_sales\": {\n        \"name\": \"sql_ticket_sales\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"write_documentation_in_markdown\": {\n        \"name\": \"write_documentation_in_markdown\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"compress_an_image\": {\n        \"name\": \"compress_an_image\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"host_your_portfolio_on_github_pages\": {\n        \"name\": \"host_your_portfolio_on_github_pages\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_google_colab\": {\n        \"name\": \"use_google_colab\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"use_an_image_library_in_google_colab\": {\n        \"name\": \"use_an_image_library_in_google_colab\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"deploy_a_python_api_to_vercel\": {\n        \"name\": \"deploy_a_python_api_to_vercel\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"create_a_github_action\": {\n        \"name\": \"create_a_github_action\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"push_an_image_to_docker_hub\": {\n        \"name\": \"push_an_image_to_docker_hub\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"write_a_fastapi_server_to_serve_data\": {\n        \"name\": \"write_a_fastapi_server_to_serve_data\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"run_a_local_llm_with_llamafile\": {\n        \"name\": \"run_a_local_llm_with_llamafile\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"llm_sentiment_analysis\": {\n        \"name\": \"llm_sentiment_analysis\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"llm_token_cost\": {\n        \"name\": \"llm_token_cost\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"generate_addresses_with_llms\": {\n        \"name\": \"generate_addresses_with_llms\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"llm_vision\": {\n        \"name\": \"llm_vision\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"llm_embeddings\": {\n        \"name\": \"llm_embeddings\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"embedding_similarity\": {\n        \"name\": \"embedding_similarity\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"vector_databases\": {\n        \"name\": \"vector_databases\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"function_calling\": {\n        \"name\": \"function_calling\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"get_an_llm_to_say_yes\": {\n        \"name\": \"get_an_llm_to_say_yes\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"import_html_to_google_sheets\": {\n        \"name\": \"import_html_to_google_sheets\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"scrape_imdb_movies\": {\n        \"name\": \"scrape_imdb_movies\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"wikipedia_outline\": {\n        \"name\": \"wikipedia_outline\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"scrape_the_bbc_weather_api\": {\n        \"name\": \"scrape_the_bbc_weather_api\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"find_the_bounding_box_of_a_city\": {\n        \"name\": \"find_the_bounding_box_of_a_city\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"search_hacker_news\": {\n        \"name\": \"search_hacker_news\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"find_newest_github_user\": {\n        \"name\": \"find_newest_github_user\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"create_a_scheduled_github_action\": {\n        \"name\": \"create_a_scheduled_github_action\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"extract_tables_from_pdf\": {\n        \"name\": \"extract_tables_from_pdf\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"convert_a_pdf_to_markdown\": {\n        \"name\": \"convert_a_pdf_to_markdown\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"clean_up_excel_sales_data\": {\n        \"name\": \"clean_up_excel_sales_data\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"clean_up_student_marks\": {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"clean_up_student_marks\",\n            \"description\": \"Analyzes logs to count the number of successful GET requests matching criteria such as URL prefix, weekday, time window, month, and year.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"file_path\": {\n                        \"type\": \"string\",\n                        \"description\": \"Path to the gzipped log file.\"\n                    },\n                    \"section_prefix\": {\n                        \"type\": \"string\",\n                        \"description\": \"URL prefix to filter log entries (e.g., '/telugu/').\"\n                    },\n                    \"weekday\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Day of the week as an integer (0=Monday, ..., 6=Sunday).\"\n                    },\n                    \"start_hour\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Start hour (inclusive) in 24-hour format.\"\n                    },\n                    \"end_hour\": {\n                        \"type\": \"integer\",\n                        \"description\": \"End hour (exclusive) in 24-hour format.\"\n                    },\n                    \"month\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Month number (e.g., 5 for May).\"\n                    },\n                    \"year\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Year (e.g., 2024).\"\n                    }\n                },\n                \"required\": [\n                    \"file_path\",\n                    \"section_prefix\",\n                    \"weekday\",\n                    \"start_hour\",\n                    \"end_hour\",\n                    \"month\",\n                    \"year\"\n                ]\n            }\n        }\n    },\n\n    \"apache_log_downloads\": {\n        \"name\": \"apache_log_downloads\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"clean_up_sales_data\": {\n        \"name\": \"clean_up_sales_data\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"parse_partial_json\": {\n        \"name\": \"parse_partial_json\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"extract_nested_json_keys\": {\n        \"name\": \"extract_nested_json_keys\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"duckdb_social_media_interactions\": {\n        \"name\": \"duckdb_social_media_interactions\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"transcribe_a_youtube_video\": {\n        \"name\": \"transcribe_a_youtube_video\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n\n    \"reconstruct_an_image\": {\n        \"name\": \"reconstruct_an_image\",\n        \"description\": \"description\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"description\": \"The text to extract the data from\"\n                }\n            },\n            \"required\": [\"text\"]\n        }\n    },\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/utils/function_definations_llm.py b/utils/function_definations_llm.py
--- a/utils/function_definations_llm.py	(revision 09f529cb700a09b44ba78bb1f7bdfcf456c37288)
+++ b/utils/function_definations_llm.py	(date 1742969061910)
@@ -672,19 +672,19 @@
     },
 
     "extract_tables_from_pdf": {
-        "name": "extract_tables_from_pdf",
-        "description": "description",
-        "parameters": {
-            "type": "object",
-            "properties": {
-                "text": {
-                    "type": "string",
-                    "description": "The text to extract the data from"
-                }
-            },
-            "required": ["text"]
-        }
-    },
+    "name": "extract_tables_from_pdf",
+    "description": "Extracts tables from a PDF and calculates the total Maths marks for students in groups 3-29 who scored 19 or more in Economics.",
+    "parameters": {
+        "type": "object",
+        "properties": {
+            "pdf_path": {
+                "type": "string",
+                "description": "The path to the PDF file to process."
+            }
+        },
+        "required": ["pdf_path"]
+    }
+},
 
     "convert_a_pdf_to_markdown": {
         "name": "convert_a_pdf_to_markdown",
Index: utils/file_process.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport zipfile\nimport tempfile\nfrom pathlib import Path\n\ndef unzip_folder(zip_path):\n    zip_path = Path(zip_path)\n\n    if not zip_path.exists():\n        raise FileNotFoundError(f\"Zip file not found: {zip_path}\")\n\n    if not zipfile.is_zipfile(zip_path):\n        # Create a temporary directory inside /data/tmp_uploads\n        base_tmp_dir = Path(\"/data/tmp_uploads\")\n        os.makedirs(base_tmp_dir, exist_ok=True)\n        temp_dir = Path(tempfile.mkdtemp(dir=base_tmp_dir))\n        temp_file_path = temp_dir / zip_path.name\n        zip_path.rename(temp_file_path)\n        return zip_path,[temp_file_path]\n\n    # Create a temporary directory inside /data/tmp_uploads\n    base_tmp_dir = Path(\"/data/tmp_uploads\")\n    os.makedirs(base_tmp_dir, exist_ok=True)\n    extract_to = Path(tempfile.mkdtemp(dir=base_tmp_dir))\n\n    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n        zip_ref.extractall(extract_to)\n\n    # also return the names of the files extracted\n    return str(extract_to), zip_ref.namelist()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/utils/file_process.py b/utils/file_process.py
--- a/utils/file_process.py	(revision 09f529cb700a09b44ba78bb1f7bdfcf456c37288)
+++ b/utils/file_process.py	(date 1742968354101)
@@ -3,28 +3,26 @@
 import tempfile
 from pathlib import Path
 
+
 def unzip_folder(zip_path):
     zip_path = Path(zip_path)
-
     if not zip_path.exists():
         raise FileNotFoundError(f"Zip file not found: {zip_path}")
 
-    if not zipfile.is_zipfile(zip_path):
-        # Create a temporary directory inside /data/tmp_uploads
-        base_tmp_dir = Path("/data/tmp_uploads")
-        os.makedirs(base_tmp_dir, exist_ok=True)
+    # Use a local directory (tmp_uploads) instead of /data/tmp_uploads
+    base_tmp_dir = Path("tmp_uploads")
+    os.makedirs(base_tmp_dir, exist_ok=True)
+
+    # If the file is not a valid zip file, simply move it into the temporary directory
+    if not zipfile.is_zipfile(zip_path):
         temp_dir = Path(tempfile.mkdtemp(dir=base_tmp_dir))
         temp_file_path = temp_dir / zip_path.name
         zip_path.rename(temp_file_path)
-        return zip_path,[temp_file_path]
+        return str(temp_file_path), [temp_file_path.name]
 
-    # Create a temporary directory inside /data/tmp_uploads
-    base_tmp_dir = Path("/data/tmp_uploads")
-    os.makedirs(base_tmp_dir, exist_ok=True)
+    # For a valid zip file, create a temporary directory to extract the contents
     extract_to = Path(tempfile.mkdtemp(dir=base_tmp_dir))
-
     with zipfile.ZipFile(zip_path, "r") as zip_ref:
         zip_ref.extractall(extract_to)
-
-    # also return the names of the files extracted
-    return str(extract_to), zip_ref.namelist()
+        file_names = zip_ref.namelist()
+    return str(extract_to), file_names
\ No newline at end of file
